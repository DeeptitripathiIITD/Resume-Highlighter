import re
import requests
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2

your_token = ""


input_query="Notable achievement in Software development"          ##Input the query here 
input_resume_path="Resume_3.pdf"         ## Place the input resume in the directory and enter the name of the resume pdf
                                        ## Run the rest of the code as it is.
                                        ## Output will be saved as highlighted_{name of the input_resume}

        output=[]

output.append(output_technical_keywords)

API_URL = "https://api-inference.huggingface.co/models/google/gemma-2b-it"
headers = {"Authorization": ""}

prompt=f"Generate technical keywords related to {input_query}"

def query(payload,prompt):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output_technical_keywords = query({
	"inputs":prompt ,
},prompt)[0]["generated_text"]

output_technical_keywords= output_technical_keywords.replace("\n", "")
output_technical_keywords= re.sub(r'[^\w\s]', '', output_technical_keywords)
output_technical_keywords = output_technical_keywords.replace(prompt, "")
output.append(output_technical_keywords)

API_URL = "https://api-inference.huggingface.co/models/google/gemma-2b-it"
headers = {"Authorization": ""}

prompt=f"Write skills related to {input_query}"

def query(payload,prompt):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output_technical_keywords = query({
	"inputs":prompt ,
},prompt)[0]["generated_text"]

output_technical_keywords= output_technical_keywords.replace("\n", "")
output_technical_keywords= re.sub(r'[^\w\s]', '', output_technical_keywords)
output_technical_keywords = output_technical_keywords.replace(prompt, "")
output.append(output_technical_keywords)

API_URL = "https://api-inference.huggingface.co/models/google/gemma-2b-it"
headers = {"Authorization": ""}

prompt=f"Write positions related to {input_query}"

def query(payload,prompt):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output_technical_keywords = query({
	"inputs":prompt ,
},prompt)[0]["generated_text"]

output_technical_keywords= output_technical_keywords.replace("\n", "")
output_technical_keywords= re.sub(r'[^\w\s]', '', output_technical_keywords)
output_technical_keywords = output_technical_keywords.replace(prompt, "")
output.append(output_technical_keywords)

API_URL = "https://api-inference.huggingface.co/models/google/gemma-2b-it"
headers = {"Authorization": ""}

prompt=f"Write job profile related to {input_query}"

def query(payload,prompt):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output_technical_keywords = query({
	"inputs":prompt ,
},prompt)[0]["generated_text"]

output_technical_keywords= output_technical_keywords.replace("\n", "")
output_technical_keywords= re.sub(r'[^\w\s]', '', output_technical_keywords)
output_technical_keywords = output_technical_keywords.replace(prompt, "")
output.append(output_technical_keywords)

API_URL = "https://api-inference.huggingface.co/models/google/gemma-2b-it"
headers = {"Authorization": ""}

prompt=f"Write education background related to {input_query}"

def query(payload,prompt):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output_technical_keywords = query({
	"inputs":prompt ,
},prompt)[0]["generated_text"]

output_technical_keywords= output_technical_keywords.replace("\n", "")
output_technical_keywords= re.sub(r'[^\w\s]', '', output_technical_keywords)
output_technical_keywords = output_technical_keywords.replace(prompt, "")
output.append(output_technical_keywords)


text_to_match= " ".join(output)
import PyPDF2

def extract_text_from_pdf(pdf_path):
    pdf_text = ""
    with open(pdf_path, "rb") as f:
        pdf_reader = PyPDF2.PdfReader(f)
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            pdf_text += page.extract_text()
    return pdf_text

def extract_lines(text):
    lines = []
    temp_line = ""

    for char in text:
        if char == '.' or char == ',':
            temp_line += char
            temp_line = temp_line.replace("\n", " ")
            lines.append(temp_line.strip())
            temp_line = ""
        elif char == '•' or char == '●' or char=="#" or char==":":  # Adding '\n' for line breaks and bullet points
            temp_line = temp_line.replace("\n", " ")
            lines.append(temp_line.strip())
            temp_line = ""
        else:
            temp_line += char

    if temp_line.strip():
        temp_line = temp_line.replace("\n", " ")
        lines.append(temp_line.strip())

    return lines

# Path to your PDF file
pdf_path = input_resume_path

# Extract text from PDF
resume_text = extract_text_from_pdf(pdf_path)

# Extract lines based on full stops, commas, and bullet points
lines = extract_lines(resume_text)
print(lines)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_similarity(line, total_text):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform([total_text])
    line_tfidf = tfidf_vectorizer.transform([line])
    similarity_scores = cosine_similarity(line_tfidf, tfidf_matrix)
    return similarity_scores[0][0]  # Return the similarity score

def give_similarity_coeff(text1, text2):
    text1 = text1.replace('\n', ' ')  # Replace newline characters with space
    text2 = text2.replace('\n', ' ')  # Replace newline characters with space
    text1_lower = text1.lower()
    text2_lower = text2.lower()
    return calculate_cosine_similarity(text1_lower, text2_lower)

    import fitz  # PyMuPDF

def highlight_pdf(pdf_path, output_path, lines):
    # Open the PDF
    pdf_document = fitz.open(pdf_path)

    j = 0  # Initialize counter for lines
    for line in lines:
        j += 1  # Increment line counter
        for page_number in range(len(pdf_document)):
            page = pdf_document[page_number]
            text = line.strip()

            # Extract text from the page
            page_text = page.get_text()

            if text in page_text:
                # Find all instances of the text
                text_instances = page.search_for(text)
                for inst in text_instances:
                    # Determine the appropriate color for the rectangle based on the line count
                    if j < len(lines) // 3:               
                        color = (1, 0.647, 0)  # Orange color
                        fill=(1, 1, 0.9)
                    elif j >= len(lines) // 3 and j <= 2 * len(lines) // 3:
                        color = (1, 1, 0)  # Yellow color
                        fill=(1, 1, 0.8)
                    else:
                        color = (1, 1, 0)  # Yellow color
                        fill=(1, 1, 0.9)
                    # Draw rectangle over text as highlight with specified color
                    page.draw_rect(inst, color=color, fill=fill, width=1.5,overlay=False)
#                     highlight = page.add_highlight_annot(inst)
                    
    pdf_document.save(output_path)
    pdf_document.close()

# Usage
line_text = []
for j in range(0, int(0.2 * len(weights))):
    line_text.append(weights[j][0])
input_pdf_path = input_resume_path
output_pdf_path = "highlighted_" + input_pdf_path
highlight_pdf(input_pdf_path, output_pdf_path, line_text)


TEXT HIGHLIGHTED:
Worked within a Agile SCRUM ticketing software development environment
Managed all content on the website and in email marketing through Mailchimp,
Presented and managed the first six weeks of front-end instruction,
and coded front end pages in a CMS with HTML,
III and AP English to 200+ 10th and 11th grade students at The Renaissance School at Olympic Community of Schools; created 2 sets of objective-driven lesson plans daily,
Wrote original and revised existing curriculum for front and back end development and provided instructor feedback from a TFA perspective
Write Monthly Online Magazines in HTML and CSS (sample client
Full stack developer who has focused equally on front end and backend and is at advanced level in both; focused on AJAX techniques to load data as user moves throughout applications; ruby,
Academic Honors Society and Student Governing Body  Chairman of the Student Disciplinary Committee  Recipient of Tonya Internship Award and Stipend
Co-founded and advise the student newspaper,
Developed frontend and backend,
and HTML5
web marketing and print marketing
edited and approved patents,
and EULAs    Ashley Hall,
and parents
and Javascript
Closed the Achievement Gap in 10th Grade Writing EOC by 60% compared with a pacesetter school
TN                                May 2008  Bachelor of Arts in Philosophy (Concentration in Ancient,
communicated results to parents and administrators
History Honor Society  Order of the Gownsmen
Cum Laude  Minor in Classical Studies  Dean’s List
Four-year renewable scholarship for academic merit and leadership  Omicron Delta Kappa
Created with Adobe Photoshop and PowerPoint to create sales presentations
com/clamstew/activity-board   University of the South – College of Liberal Arts,
postgreSQL  Developed the start of a chrome extension to package up a user’s daily links
tracked and analyzed student data,
and javascript frameworks
Attended continuing educational courses and conferences,
The Scarlet Letter
Worked on multiple large scale php web applications (using plain php and Zend Framework)
managed pull requests and issues for a rails Recruit App used by graduates to apply for positions after exiting 12 week course
Facilitated the Graduation Project research paper for 180 students
